{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T07:16:16.546521Z",
     "start_time": "2025-06-24T07:16:14.215179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install scikit-learn\n",
    "!pip install opencv-python"
   ],
   "id": "932357fc3b904718",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/amberchen/PyCharmMiscProject/.venv/lib/python3.10/site-packages (1.5.2)\r\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/amberchen/PyCharmMiscProject/.venv/lib/python3.10/site-packages (from scikit-learn) (2.2.6)\r\n",
      "Requirement already satisfied: scipy>=1.6.0 in /Users/amberchen/PyCharmMiscProject/.venv/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\r\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/amberchen/PyCharmMiscProject/.venv/lib/python3.10/site-packages (from scikit-learn) (1.5.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/amberchen/PyCharmMiscProject/.venv/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\r\n",
      "Requirement already satisfied: opencv-python in /Users/amberchen/PyCharmMiscProject/.venv/lib/python3.10/site-packages (4.11.0.86)\r\n",
      "Requirement already satisfied: numpy>=1.21.2 in /Users/amberchen/PyCharmMiscProject/.venv/lib/python3.10/site-packages (from opencv-python) (2.2.6)\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T07:16:17.911256Z",
     "start_time": "2025-06-24T07:16:16.554258Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T07:16:17.919392Z",
     "start_time": "2025-06-24T07:16:17.916022Z"
    }
   },
   "cell_type": "code",
   "source": "os.chdir('/Volumes/T7/drive_thesis/thesis/img/cropped/1200/n_3')",
   "id": "d9e6f89c5959ac7e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T07:16:18.087587Z",
     "start_time": "2025-06-24T07:16:17.924939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "source_crop = [\n",
    "    'cropped_real_300x300.h5',\n",
    "    'cropped_synthetic_300x300.h5'\n",
    "]\n",
    "\n",
    "source_crop_1200 = [\n",
    "    'cropped_real_1200x1200.h5',\n",
    "    'cropped_synthetic_1200x1200.h5'\n",
    "]\n",
    "\n",
    "combine_crop_n1 = [\n",
    "    '/Volumes/T7/drive_thesis/thesis/img/cropped/n_1/combined_images_vds_300.h5',\n",
    "    '/Volumes/T7/drive_thesis/thesis/img/converted/combined_images_vds_300.h5'\n",
    "]\n",
    "\n",
    "combine_crop_n2 = [\n",
    "    '/Volumes/T7/drive_thesis/thesis/img/cropped/n_2/combined_images_vds_300.h5',\n",
    "    '/Volumes/T7/drive_thesis/thesis/img/converted/combined_images_vds_300.h5'\n",
    "]\n",
    "\n",
    "combine_crop_n3 = [\n",
    "    '/Volumes/T7/drive_thesis/thesis/img/cropped/n_3/combined_images_vds_300.h5',\n",
    "    '/Volumes/T7/drive_thesis/thesis/img/converted/combined_images_vds_300.h5'\n",
    "]\n",
    "\n",
    "combine_crop_n5 = [\n",
    "    '/Volumes/T7/drive_thesis/thesis/img/cropped/n_5/combined_images_vds_300.h5',\n",
    "    '/Volumes/T7/drive_thesis/thesis/img/converted/combined_images_vds_300.h5'\n",
    "]\n",
    "\n",
    "combine_crop_1200_n1 = [\n",
    "    '/Volumes/T7/drive_thesis/thesis/img/cropped/1200/n_1/combined_images_vds_1200.h5',\n",
    "    '/Volumes/T7/drive_thesis/thesis/img/converted/combined_images_vds_1200.h5'\n",
    "]\n",
    "\n",
    "combine_crop_1200_n2 = [\n",
    "    '/Volumes/T7/drive_thesis/thesis/img/cropped/1200/n_2/combined_images_vds_1200.h5',\n",
    "    '/Volumes/T7/drive_thesis/thesis/img/converted/combined_images_vds_1200.h5'\n",
    "]\n",
    "\n",
    "combine_crop_1200_n3 = [\n",
    "    '/Volumes/T7/drive_thesis/thesis/img/cropped/1200/n_3/combined_images_vds_1200.h5',\n",
    "    '/Volumes/T7/drive_thesis/thesis/img/converted/combined_images_vds_1200.h5'\n",
    "]\n",
    "\n",
    "source_1200 = [\n",
    "    'ag_real_1200x1200.h5',\n",
    "    'ag_synthetic_1200x1200.h5',\n",
    "    'chen_real_1200x1200.h5',\n",
    "    'chen_synthetic_1200x1200.h5'\n",
    "]\n",
    "\n",
    "source_600 = [\n",
    "    'ag_real_600x600.h5',\n",
    "    'ag_synthetic_600x600.h5',\n",
    "    'chen_real_600x600.h5',\n",
    "    'chen_synthetic_600x600.h5'\n",
    "]\n",
    "\n",
    "source_300 = [\n",
    "    'ag_real_300x300.h5',\n",
    "    'ag_synthetic_300x300.h5',\n",
    "    'chen_real_300x300.h5',\n",
    "    'chen_synthetic_300x300.h5'\n",
    "]\n",
    "\n",
    "source_200 = [\n",
    "    'ag_real_200x200.h5',\n",
    "    'ag_synthetic_200x200.h5',\n",
    "    'chen_real_200x200.h5',\n",
    "    'chen_synthetic_200x200.h5'\n",
    "]\n",
    "\n",
    "source_100 = [\n",
    "    'ag_real_100x100.h5',\n",
    "    'ag_synthetic_100x100.h5',\n",
    "    'chen_real_100x100.h5',\n",
    "    'chen_synthetic_100x100.h5'\n",
    "]\n",
    "\n",
    "\n",
    "source_h5_files = source_crop_1200\n",
    "\n",
    "source_labels = [0, 1]\n",
    "size = 1200\n",
    "\n",
    "target_vds_h5_path = f'combined_images_vds_{size}.h5'\n",
    "dataset_name_in_source = 'images'\n",
    "label_dataset_name = 'labels'"
   ],
   "id": "8d749deec44bd738",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T07:16:18.559434Z",
     "start_time": "2025-06-24T07:16:18.093430Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 64\n",
    "total_images = 0\n",
    "label_arrays = []\n",
    "image_shape = None\n",
    "\n",
    "print(\"Reading source file info and collecting labels...\")\n",
    "for i, source_file in enumerate(source_h5_files):\n",
    "    try:\n",
    "        with h5py.File(source_file, 'r') as hf_source:\n",
    "            if dataset_name_in_source not in hf_source:\n",
    "                print(f\"Error: Dataset '{dataset_name_in_source}' not found in {source_file}\")\n",
    "                continue\n",
    "\n",
    "            source_dataset = hf_source[dataset_name_in_source]\n",
    "            num_images_in_file = source_dataset.shape[0]\n",
    "            current_image_shape = source_dataset.shape[1:]\n",
    "\n",
    "            print(f\"  File: {source_file}, Images: {num_images_in_file}, Shape: {current_image_shape}, Dtype: {source_dataset.dtype}\")\n",
    "\n",
    "            if image_shape is None:\n",
    "                image_shape = current_image_shape\n",
    "            elif image_shape != current_image_shape:\n",
    "                print(f\"Warning: Image shape mismatch! Expected {image_shape}, found {current_image_shape} in {source_file}. VDS requires consistent image shapes.\")\n",
    "\n",
    "            label = source_labels[i]\n",
    "            labels_for_file = np.full((num_images_in_file,), label, dtype=np.int64)\n",
    "            label_arrays.append(labels_for_file)\n",
    "\n",
    "            total_images += num_images_in_file\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Source file not found: {source_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {source_file}: {e}\")\n",
    "\n",
    "\n",
    "if label_arrays:\n",
    "    y = np.concatenate(label_arrays, axis=0)\n",
    "    print(f\"\\nSuccessfully concatenated labels. Total labels (y) shape: {y.shape}, Dtype: {y.dtype}\")\n",
    "else:\n",
    "    y = np.array([], dtype=np.int64)\n",
    "    print(\"\\nNo label data collected.\")\n",
    "\n",
    "\n",
    "if total_images == 0 or image_shape is None:\n",
    "    print(\"\\nCannot create VDS: No images found or image shape is unknown.\")\n",
    "else:\n",
    "    print(f\"\\nDefining VDS layout for total images: {total_images}, image shape: {image_shape}\")\n",
    "\n",
    "    vds_shape = (total_images,) + image_shape\n",
    "\n",
    "    layout = h5py.VirtualLayout(shape=vds_shape, dtype=np.uint8)\n",
    "\n",
    "    current_vds_offset = 0\n",
    "    print(\"Mapping source datasets to VDS layout:\")\n",
    "    for i, source_file in enumerate(source_h5_files):\n",
    "        try:\n",
    "            with h5py.File(source_file, 'r') as hf_source:\n",
    "                 if dataset_name_in_source not in hf_source:\n",
    "                     continue\n",
    "\n",
    "                 source_dataset = hf_source[dataset_name_in_source]\n",
    "                 num_images_in_file = source_dataset.shape[0]\n",
    "\n",
    "                 vsource = h5py.VirtualSource(\n",
    "                     source_file,\n",
    "                     dataset_name_in_source,\n",
    "                     shape=source_dataset.shape\n",
    "                 )\n",
    "\n",
    "                 vds_slice = slice(current_vds_offset, current_vds_offset + num_images_in_file)\n",
    "\n",
    "                 layout[vds_slice, ...] = vsource[0:num_images_in_file, ...]\n",
    "\n",
    "                 print(f\"  Mapped {source_file}[{dataset_name_in_source}] to VDS slice {vds_slice}\")\n",
    "\n",
    "                 current_vds_offset += num_images_in_file\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while defining VDS layout for {source_file}: {e}\")\n",
    "\n",
    "\n",
    "    print(f\"\\nCreating target HDF5 file with VDS: {target_vds_h5_path}\")\n",
    "    try:\n",
    "        with h5py.File(target_vds_h5_path, 'w', libver='latest') as hf_target:\n",
    "            hf_target.create_virtual_dataset('images_vds', layout)\n",
    "\n",
    "            if y.size > 0:\n",
    "                 hf_target.create_dataset(label_dataset_name, data=y)\n",
    "                 print(f\"Saved concatenated labels as dataset '{label_dataset_name}' in target file.\")\n",
    "\n",
    "            print(f\"Successfully created target HDF5 file with VDS: {target_vds_h5_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while creating the target HDF5 file or VDS: {e}\")\n",
    "\n",
    "print(\"\\n--- Demonstration of accessing data via VDS ---\")\n",
    "if os.path.exists(target_vds_h5_path):\n",
    "    try:\n",
    "        with h5py.File(target_vds_h5_path, 'r') as hf_vds:\n",
    "            if 'images_vds' in hf_vds:\n",
    "                vds_dataset = hf_vds['images_vds']\n",
    "\n",
    "                print(f\"Accessed Virtual Dataset 'images_vds'.\")\n",
    "                print(f\"VDS Shape: {vds_dataset.shape}\")\n",
    "                print(f\"VDS Dtype: {vds_dataset.dtype}\")\n",
    "\n",
    "                if vds_dataset.shape[0] > 0:\n",
    "                     first_image_vds = vds_dataset[0]\n",
    "                     print(f\"  Read first image from VDS. Shape: {first_image_vds.shape}, Dtype: {first_image_vds.dtype}\")\n",
    "\n",
    "                batch_start = total_images // 2 if total_images > 0 else 0\n",
    "                batch_end = batch_start + batch_size if total_images > 0 else 0\n",
    "\n",
    "                if total_images > 0 and batch_start < total_images and batch_start < batch_end and batch_end <= total_images:\n",
    "                    middle_batch_vds = vds_dataset[batch_start:batch_end]\n",
    "                    print(f\"  Read batch from VDS [{batch_start}:{batch_end}]. Shape: {middle_batch_vds.shape}, Dtype: {middle_batch_vds.dtype}\")\n",
    "\n",
    "            else:\n",
    "                print(\"Error: 'images_vds' dataset not found in the target HDF5 file.\")\n",
    "\n",
    "            if label_dataset_name in hf_vds:\n",
    "                loaded_labels_vds_file = hf_vds[label_dataset_name]\n",
    "                print(f\"Accessed concatenated labels dataset '{label_dataset_name}'.\")\n",
    "                print(f\"Loaded labels shape: {loaded_labels_vds_file.shape}, Dtype: {loaded_labels_vds_file.dtype}\")\n",
    "\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Target VDS file not found: {target_vds_h5_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while accessing data from VDS: {e}\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\nTarget VDS file '{target_vds_h5_path}' was not created successfully.\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Verification ---\")\n",
    "if 'y' in locals():\n",
    "    print(f\"Final concatenated labels (y) shape: {y.shape}\")\n",
    "if os.path.exists(target_vds_h5_path):\n",
    "    try:\n",
    "        with h5py.File(target_vds_h5_path, 'r') as hf_vds:\n",
    "             if 'images_vds' in hf_vds:\n",
    "                 vds_dataset = hf_vds['images_vds']\n",
    "                 print(f\"Final Virtual Dataset (images_vds) shape: {vds_dataset.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error verifying VDS shape: {e}\")"
   ],
   "id": "edd9a73b19f3b397",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading source file info and collecting labels...\n",
      "  File: cropped_real_1200x1200.h5, Images: 3708, Shape: (1200, 1200, 3), Dtype: uint8\n",
      "  File: cropped_synthetic_1200x1200.h5, Images: 920, Shape: (1200, 1200, 3), Dtype: uint8\n",
      "\n",
      "Successfully concatenated labels. Total labels (y) shape: (4628,), Dtype: int64\n",
      "\n",
      "Defining VDS layout for total images: 4628, image shape: (1200, 1200, 3)\n",
      "Mapping source datasets to VDS layout:\n",
      "  Mapped cropped_real_1200x1200.h5[images] to VDS slice slice(0, 3708, None)\n",
      "  Mapped cropped_synthetic_1200x1200.h5[images] to VDS slice slice(3708, 4628, None)\n",
      "\n",
      "Creating target HDF5 file with VDS: combined_images_vds_1200.h5\n",
      "Saved concatenated labels as dataset 'labels' in target file.\n",
      "Successfully created target HDF5 file with VDS: combined_images_vds_1200.h5\n",
      "\n",
      "--- Demonstration of accessing data via VDS ---\n",
      "Accessed Virtual Dataset 'images_vds'.\n",
      "VDS Shape: (4628, 1200, 1200, 3)\n",
      "VDS Dtype: uint8\n",
      "  Read first image from VDS. Shape: (1200, 1200, 3), Dtype: uint8\n",
      "  Read batch from VDS [2314:2378]. Shape: (64, 1200, 1200, 3), Dtype: uint8\n",
      "Accessed concatenated labels dataset 'labels'.\n",
      "Loaded labels shape: (4628,), Dtype: int64\n",
      "\n",
      "--- Verification ---\n",
      "Final concatenated labels (y) shape: (4628,)\n",
      "Final Virtual Dataset (images_vds) shape: (4628, 1200, 1200, 3)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T07:16:19.071889Z",
     "start_time": "2025-06-24T07:16:19.057004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_train_test_split_from_vds(\n",
    "    hdf5_path,\n",
    "    test_size,\n",
    "    output_dir='.',\n",
    "    random_state=None,\n",
    "    image_dataset_name='images_vds',\n",
    "    label_dataset_name='labels',\n",
    "    output_h5_filename='train_test_split.h5',\n",
    "    label_chunk_size=10000\n",
    "):\n",
    "\n",
    "    if output_dir and not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        print(f\"Created output directory: {output_dir}\")\n",
    "\n",
    "    output_h5_path = os.path.join(output_dir, output_h5_filename)\n",
    "\n",
    "    print(f\"Attempting to load data from {hdf5_path} and save split to {output_h5_path}...\")\n",
    "\n",
    "    try:\n",
    "        with h5py.File(hdf5_path, 'r') as hf_in:\n",
    "            if image_dataset_name not in hf_in:\n",
    "                print(f\"Error: Image dataset '{image_dataset_name}' not found in {hf_in.filename}\")\n",
    "                return\n",
    "\n",
    "            if label_dataset_name not in hf_in:\n",
    "                 print(f\"Error: Label dataset '{label_dataset_name}' not found in {hf_in.filename}\")\n",
    "                 return\n",
    "\n",
    "            images_vds = hf_in[image_dataset_name]\n",
    "            labels_dataset = hf_in[label_dataset_name]\n",
    "\n",
    "            print(f\"Successfully accessed '{image_dataset_name}' (Shape: {images_vds.shape}, Dtype: {images_vds.dtype})\")\n",
    "            print(f\"Successfully accessed '{label_dataset_name}' (Shape: {labels_dataset.shape}, Dtype: {labels_dataset.dtype})\")\n",
    "\n",
    "            if images_vds.shape[0] != labels_dataset.shape[0]:\n",
    "                print(f\"Error: Number of images ({images_vds.shape[0]}) and labels ({labels_dataset.shape[0]}) do not match.\")\n",
    "                return\n",
    "\n",
    "            total_samples = images_vds.shape[0]\n",
    "            print(f\"Total samples found: {total_samples}\")\n",
    "\n",
    "\n",
    "            print(\"Reading labels in chunks to determine class distribution for stratification...\")\n",
    "            label_counts = defaultdict(int)\n",
    "            label_indices_by_class = defaultdict(list)\n",
    "\n",
    "            for i in range(0, total_samples, label_chunk_size):\n",
    "                end_idx = min(i + label_chunk_size, total_samples)\n",
    "                label_chunk = labels_dataset[i:end_idx]\n",
    "                current_indices = np.arange(i, end_idx)\n",
    "\n",
    "                for label, index in zip(label_chunk, current_indices):\n",
    "                    label_counts[label] += 1\n",
    "                    label_indices_by_class[label].append(index)\n",
    "\n",
    "            print(\"Label counts by class:\", dict(label_counts))\n",
    "\n",
    "            train_indices = []\n",
    "            test_indices = []\n",
    "\n",
    "            for label, indices in label_indices_by_class.items():\n",
    "                if len(indices) > 0:\n",
    "                    current_test_size = test_size\n",
    "                    if isinstance(test_size, float):\n",
    "                         current_test_size = max(1, int(len(indices) * test_size))\n",
    "                    elif isinstance(test_size, int):\n",
    "                         current_test_size = min(test_size, len(indices))\n",
    "\n",
    "                    if current_test_size == len(indices):\n",
    "                         print(f\"Warning: All samples in class {label} ({len(indices)}) will be put in the test set as test_size is equal to class size.\")\n",
    "                         class_train_indices = []\n",
    "                         class_test_indices = indices\n",
    "                    elif current_test_size == 0:\n",
    "                         print(f\"Warning: No samples from class {label} will be in the test set as test_size is 0 or calculated test_size is 0.\")\n",
    "                         class_train_indices = indices\n",
    "                         class_test_indices = []\n",
    "\n",
    "                    elif len(indices) < current_test_size:\n",
    "                         print(f\"Warning: Not enough samples ({len(indices)}) in class {label} to meet requested test_size ({current_test_size}). All available will be used for split.\")\n",
    "                         class_train_indices, class_test_indices = train_test_split(\n",
    "                            indices,\n",
    "                            test_size=current_test_size,\n",
    "                            random_state=random_state,\n",
    "                            shuffle=True\n",
    "                         )\n",
    "\n",
    "                    else:\n",
    "                        class_train_indices, class_test_indices = train_test_split(\n",
    "                            indices,\n",
    "                            test_size=current_test_size,\n",
    "                            random_state=random_state,\n",
    "                            shuffle=True\n",
    "                        )\n",
    "\n",
    "                    train_indices.extend(class_train_indices)\n",
    "                    test_indices.extend(class_test_indices)\n",
    "                else:\n",
    "                    print(f\"Warning: No samples found for class {label}.\")\n",
    "\n",
    "            train_indices = np.array(train_indices)\n",
    "            test_indices = np.array(test_indices)\n",
    "\n",
    "            np.random.shuffle(train_indices, random_state=42)\n",
    "            np.random.shuffle(test_indices, random_state=42)\n",
    "\n",
    "\n",
    "            print(f\"Train indices shape: {train_indices.shape}\")\n",
    "            print(f\"Test indices shape: {test_indices.shape}\")\n",
    "\n",
    "            print(f\"\\nSaving train/test sets to {output_h5_path}...\")\n",
    "\n",
    "            try:\n",
    "                with h5py.File(output_h5_path, 'w') as hf_out:\n",
    "                    print(\"Loading and saving training data...\")\n",
    "                    train_indices_sorted = np.sort(train_indices)\n",
    "                    batch_size_save = 1000\n",
    "                    for i in range(0, len(train_indices_sorted), batch_size_save):\n",
    "                        end_idx_save = min(i + batch_size_save, len(train_indices_sorted))\n",
    "                        current_indices_to_load = train_indices_sorted[i:end_idx_save]\n",
    "\n",
    "                        X_train_batch = images_vds[current_indices_to_load]\n",
    "                        y_train_batch = labels_dataset[current_indices_to_load]\n",
    "\n",
    "                        if i == 0:\n",
    "                             hf_out.create_dataset('X_train', data=X_train_batch, maxshape=(None,) + X_train_batch.shape[1:], dtype=X_train_batch.dtype)\n",
    "                             hf_out.create_dataset('y_train', data=y_train_batch, maxshape=(None,), dtype=y_train_batch.dtype)\n",
    "                        else:\n",
    "                             hf_out['X_train'].resize((hf_out['X_train'].shape[0] + X_train_batch.shape[0]), axis=0)\n",
    "                             hf_out['X_train'][-X_train_batch.shape[0]:] = X_train_batch\n",
    "\n",
    "                             hf_out['y_train'].resize((hf_out['y_train'].shape[0] + y_train_batch.shape[0]), axis=0)\n",
    "                             hf_out['y_train'][-y_train_batch.shape[0]:] = y_train_batch\n",
    "\n",
    "                        print(f\"  Saved batch {i//batch_size_save + 1}. Current X_train size: {hf_out['X_train'].shape[0]}\")\n",
    "\n",
    "\n",
    "                    print(f\"Finished saving X_train (Shape: {hf_out['X_train'].shape}) and y_train (Shape: {hf_out['y_train'].shape})\")\n",
    "\n",
    "\n",
    "                    print(\"Loading and saving testing data...\")\n",
    "                    test_indices_sorted = np.sort(test_indices)\n",
    "                    for i in range(0, len(test_indices_sorted), batch_size_save):\n",
    "                         end_idx_save = min(i + batch_size_save, len(test_indices_sorted))\n",
    "                         current_indices_to_load = test_indices_sorted[i:end_idx_save]\n",
    "\n",
    "                         X_test_batch = images_vds[current_indices_to_load]\n",
    "                         y_test_batch = labels_dataset[current_indices_to_load]\n",
    "\n",
    "                         if i == 0:\n",
    "                             hf_out.create_dataset('X_test', data=X_test_batch, maxshape=(None,) + X_test_batch.shape[1:], dtype=X_test_batch.dtype)\n",
    "                             hf_out.create_dataset('y_test', data=y_test_batch, maxshape=(None,), dtype=y_test_batch.dtype)\n",
    "                         else:\n",
    "                             hf_out['X_test'].resize((hf_out['X_test'].shape[0] + X_test_batch.shape[0]), axis=0)\n",
    "                             hf_out['X_test'][-X_test_batch.shape[0]:] = X_test_batch\n",
    "\n",
    "                             hf_out['y_test'].resize((hf_out['y_test'].shape[0] + y_test_batch.shape[0]), axis=0)\n",
    "                             hf_out['y_test'][-y_test_batch.shape[0]:] = y_test_batch\n",
    "\n",
    "                         print(f\"  Saved batch {i//batch_size_save + 1}. Current X_test size: {hf_out['X_test'].shape[0]}\")\n",
    "\n",
    "                    print(f\"Finished saving X_test (Shape: {hf_out['X_test'].shape}) and y_test (Shape: {hf_out['y_test'].shape})\")\n",
    "\n",
    "                print(\"Train/test split successfully saved to HDF5.\")\n",
    "\n",
    "            except Exception as save_e:\n",
    "                print(f\"An error occurred while saving train/test sets to HDF5: {save_e}\")\n",
    "\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input HDF5 file not found at {hdf5_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during train/test split process: {e}\")\n",
    "\n",
    "    print(\"Train/test split process finished.\")"
   ],
   "id": "b7c9b52752708c4c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T07:29:11.035551Z",
     "start_time": "2025-06-24T07:25:54.381118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "hdf5_path = f'/Volumes/T7/drive_thesis/thesis/img/cropped/1200/n_3/combined_images_vds_1200.h5'\n",
    "test_size = 0.2\n",
    "random_state = 42\n",
    "output_dir = f'/Volumes/T7/drive_thesis/thesis/npy/cropped/1200/n_3'\n",
    "\n",
    "create_train_test_split_from_vds(\n",
    "    hdf5_path,\n",
    "    test_size,\n",
    "    output_dir=output_dir,\n",
    "    random_state=random_state,\n",
    "    image_dataset_name='images_vds',\n",
    "    label_dataset_name='labels',\n",
    "    output_h5_filename='train_test_split_crop.h5'\n",
    ")"
   ],
   "id": "49b6a28384da7466",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load data from /Volumes/T7/drive_thesis/thesis/img/cropped/1200/n_3/combined_images_vds_1200.h5 and save split to /Volumes/T7/drive_thesis/thesis/npy/cropped/1200/n_3/train_test_split_crop.h5...\n",
      "Successfully accessed 'images_vds' (Shape: (4628, 1200, 1200, 3), Dtype: uint8)\n",
      "Successfully accessed 'labels' (Shape: (4628,), Dtype: int64)\n",
      "Total samples found: 4628\n",
      "Reading labels in chunks to determine class distribution for stratification...\n",
      "Label counts by class: {np.int64(0): 3708, np.int64(1): 920}\n",
      "Train indices shape: (3703,)\n",
      "Test indices shape: (925,)\n",
      "\n",
      "Saving train/test sets to /Volumes/T7/drive_thesis/thesis/npy/cropped/1200/n_3/train_test_split_crop.h5...\n",
      "Loading and saving training data...\n",
      "  Saved batch 1. Current X_train size: 1000\n",
      "  Saved batch 2. Current X_train size: 2000\n",
      "  Saved batch 3. Current X_train size: 3000\n",
      "  Saved batch 4. Current X_train size: 3703\n",
      "Finished saving X_train (Shape: (3703, 1200, 1200, 3)) and y_train (Shape: (3703,))\n",
      "Loading and saving testing data...\n",
      "  Saved batch 1. Current X_test size: 925\n",
      "Finished saving X_test (Shape: (925, 1200, 1200, 3)) and y_test (Shape: (925,))\n",
      "Train/test split successfully saved to HDF5.\n",
      "Train/test split process finished.\n"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
